# HELM Model Deployments Configuration
# Configures how HELM connects to the LLM via our proxy

model_deployments:
  - name: openai/smollm3-3b
    model_name: openai/smollm3-3b
    tokenizer_name: simple/tokenizer1
    max_sequence_length: 4096
    client_spec:
      class_name: "helm.clients.openai_client.OpenAIClient"
      args:
        api_key: "dummy"
        org_id: ""
        # Points to our llm_proxy container
        base_url: "http://llm_proxy:8000/v1/"
        openai_model_name: "smollm3-3b"
